{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40257f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "#from data_utils import *\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import warnings\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning, message='.*OneHotEncoder was fitted without feature names.*')\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataframe, target_cols, tokenizer,type_text, max_token_len=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.type_text = type_text\n",
    "        self.dataframe = dataframe\n",
    "        self.labels = dataframe[target_cols].values\n",
    "        self.max_token_len = max_token_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_row = self.dataframe.iloc[idx]\n",
    "\n",
    "        text_data = data_row[self.type_text]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text_data,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_token_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        text = encoding['input_ids'].flatten()\n",
    "        att_mask = encoding['attention_mask'].flatten()\n",
    "        labels = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return text, att_mask, labels\n",
    "\n",
    "\n",
    "class DemographicsDataset(Dataset):\n",
    "    def __init__(self, dataframe, labels):\n",
    "        self.features = dataframe.values \n",
    "        self.labels = labels  \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        demo = torch.tensor(self.features[idx], dtype=torch.float)\n",
    "        labels = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return demo, labels\n",
    "\n",
    "class MedicalImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, target_cols, img_col, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.img_col = img_col\n",
    "        self.labels = dataframe[target_cols].values\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        labels = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        img_path = self.dataframe.iloc[idx][self.img_col]\n",
    "        image = Image.open(img_path).convert('RGB')  # Convert to RGB for consistency\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, labels\n",
    "\n",
    "    \n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, time_series_data):\n",
    "        self.time_series_data = time_series_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.time_series_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        time_series = self.time_series_data[idx]\n",
    "        ts = torch.tensor(time_series, dtype=torch.float)\n",
    "        return ts\n",
    "\n",
    "def regular_sample_sequence(seq, fixed_length):\n",
    "    n = len(seq)\n",
    "    if n > fixed_length:\n",
    "        # Select indices at regular intervals\n",
    "        indices = np.round(np.linspace(0, n - 1, fixed_length)).astype(int)\n",
    "        sampled_seq = seq[indices]\n",
    "    else:\n",
    "        # If shorter, pad the sequence\n",
    "        padded_seq = np.zeros((fixed_length, seq.shape[1]))\n",
    "        padded_seq[:n] = seq\n",
    "        sampled_seq = padded_seq\n",
    "    return sampled_seq\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_timeseries(file_path, config, encoders): #, fixed_length\n",
    "    categorical_cols = [col for col in config[\"id_to_channel\"] if config[\"is_categorical_channel\"][col]]\n",
    "    continuous_cols = [col for col in config[\"id_to_channel\"] if not config[\"is_categorical_channel\"][col]]\n",
    "\n",
    "    ts = pd.read_csv(file_path)\n",
    "\n",
    "    # Impute missing values\n",
    "    ts[continuous_cols] = ts[continuous_cols].fillna(method='ffill').fillna(0)\n",
    "    ts[categorical_cols] = ts[categorical_cols].fillna('missing')\n",
    "    \n",
    "    # Initialize a DataFrame for the encoded data\n",
    "    encoded_data = pd.DataFrame(index=ts.index)\n",
    "\n",
    "    # One-hot encode each categorical column separately\n",
    "    for col in categorical_cols:\n",
    "        if col in encoders:  # Check if the encoder for the column exists\n",
    "            encoded_col = encoders[col].transform(ts[[col]])\n",
    "            encoded_col_df = pd.DataFrame(encoded_col, columns=encoders[col].get_feature_names_out([col]), index=ts.index)\n",
    "            encoded_data = pd.concat([encoded_data, encoded_col_df], axis=1)\n",
    "\n",
    "    ts = ts.drop(columns=categorical_cols)\n",
    "    ts = pd.concat([ts, encoded_data], axis=1)\n",
    "    \n",
    "    # Normalize continuous variables\n",
    "    scaler = StandardScaler()\n",
    "    ts[continuous_cols] = scaler.fit_transform(ts[continuous_cols])\n",
    "    \n",
    "    return ts\n",
    "\n",
    "\n",
    "def pad_sequence(seq, maxlen, n_features):\n",
    "    padded_seq = np.zeros((maxlen, n_features))\n",
    "    padded_seq[:len(seq)] = seq[:maxlen]\n",
    "    return padded_seq\n",
    "\n",
    "\n",
    "def preprocess_demo(df, categorical_encoders=None):\n",
    "    # Specify demographic columns\n",
    "    demographic_cols = ['admittime', 'admission_type', 'admission_location', \n",
    "                        'insurance', 'language', 'marital_status', 'ethnicity', \n",
    "                        'gender', 'anchor_age', 'anchor_year', 'anchor_year_group']\n",
    "    \n",
    "    df = df[demographic_cols].copy()\n",
    "\n",
    "    # Initially identify categorical and continuous columns\n",
    "    continuous_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    categorical_cols = [col for col in df.select_dtypes(include=['object']).columns if col != 'admittime']\n",
    "    print(categorical_cols)\n",
    "\n",
    "    # Convert 'admittime' to datetime and extract features\n",
    "    df['admittime'] = pd.to_datetime(df['admittime'])\n",
    "    df['admission_hour'] = df['admittime'].dt.hour\n",
    "    df['admission_dayofweek'] = df['admittime'].dt.dayofweek\n",
    "    df['admission_month'] = df['admittime'].dt.month\n",
    "    \n",
    "    # Drop the original 'admittime' column after feature extraction\n",
    "    df.drop(columns=['admittime'], inplace=True)\n",
    "\n",
    "    # Impute missing values for continuous and categorical columns\n",
    "    imputer_continuous = SimpleImputer(strategy='mean')\n",
    "    imputer_categorical = SimpleImputer(strategy='most_frequent')\n",
    "    df[continuous_cols] = imputer_continuous.fit_transform(df[continuous_cols])\n",
    "    for col in categorical_cols:\n",
    "        df[col] = imputer_categorical.fit_transform(df[[col]]) #.ravel()\n",
    "    \n",
    "    # One-hot encode categorical variables\n",
    "    if categorical_encoders is None:\n",
    "        categorical_encoders = {}\n",
    "        for col in categorical_cols:\n",
    "            encoder = OneHotEncoder(handle_unknown='ignore', sparse =False)\n",
    "            categorical_encoders[col] = encoder.fit(df[[col]])\n",
    "            transformed = encoder.transform(df[[col]])\n",
    "            df = df.drop(columns=[col])\n",
    "            df = pd.concat([df, pd.DataFrame(transformed, columns=encoder.get_feature_names_out([col]))], axis=1)\n",
    "    else:\n",
    "        for col in categorical_cols:\n",
    "            encoder = categorical_encoders[col]\n",
    "            transformed = encoder.transform(df[[col]])\n",
    "            df = df.drop(columns=[col])\n",
    "            df = pd.concat([df, pd.DataFrame(transformed, columns=encoder.get_feature_names_out([col]))], axis=1)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    df[continuous_cols] = scaler.fit_transform(df[continuous_cols])\n",
    "\n",
    "    return df, categorical_encoders\n",
    "\n",
    "def create_demo_general(path_to_core):\n",
    "    pts = pd.read_csv(path_to_core + \"patients.csv\")\n",
    "    admissions = pd.read_csv(path_to_core + \"admissions.csv\")\n",
    "    demo = admissions.merge(pts, on = \"subject_id\")\n",
    "    demo = demo[['subject_id', 'hadm_id', 'admittime',\n",
    "       'admission_type', 'admission_location',\n",
    "       'insurance', 'language', 'marital_status', 'ethnicity', 'gender', 'anchor_age',\n",
    "       'anchor_year', 'anchor_year_group']]\n",
    "    return demo\n",
    "\n",
    "def create_time_series_dataset(file_paths, config, encoders, fixed_length):\n",
    "    processed_data = [preprocess_timeseries(path, config, encoders) for path in file_paths]\n",
    "    sampled_data = [regular_sample_sequence(data.values, fixed_length) for data in processed_data]\n",
    "    print(sampled_data[0].shape)\n",
    "    print(sampled_data[20].shape)\n",
    "    \n",
    "    return TimeSeriesDataset(sampled_data)\n",
    "\n",
    "def create_demo_dataset(pre_processed_data, target_cols):\n",
    "    return DemographicsDataset(pre_processed_data, target_cols)\n",
    "\n",
    "def create_image_dataset(data, target_cols, img_col, transform):\n",
    "    # Initialize the image dataset\n",
    "    return MedicalImageDataset(data, target_cols, img_col, transform)\n",
    "\n",
    "def create_text_dataset(data, target_cols, type_text):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"medicalai/ClinicalBERT\")\n",
    "    return TextDataset(data,target_cols, tokenizer, type_text)\n",
    "\n",
    "def determine_categories_and_length(ehr_data_dir, task, phase, data, config):\n",
    "    categorical_cols = [col for col in config[\"id_to_channel\"] if config[\"is_categorical_channel\"][col]]\n",
    "\n",
    "    all_categories = {col: set() for col in categorical_cols}\n",
    "    lengths = []\n",
    "    for file_path in data['filename']:\n",
    "        full_path = f'{ehr_data_dir}/{task}/{phase}/' + file_path\n",
    "        ts = pd.read_csv(full_path, usecols=categorical_cols)\n",
    "        lengths.append(len(ts))\n",
    "        for col in categorical_cols:\n",
    "            all_categories[col].update(ts[col].dropna().unique())\n",
    "\n",
    "    fixed_length = int(np.mean(lengths))\n",
    "\n",
    "    return all_categories, fixed_length\n",
    "\n",
    "def create_encoders(unique_categories):\n",
    "    encoders = {}\n",
    "    for col, categories in unique_categories.items():\n",
    "        if not categories:\n",
    "            print(f\"No unique values found for column '{col}'. Skipping this column.\")\n",
    "            continue\n",
    "\n",
    "        unique_values = list(categories)\n",
    "        encoder = OneHotEncoder(categories=[unique_values], handle_unknown='ignore', sparse=False)\n",
    "        encoder.fit(np.array(unique_values).reshape(-1, 1))\n",
    "\n",
    "        encoders[col] = encoder\n",
    "\n",
    "    return encoders\n",
    "\n",
    "\n",
    "def save_dataset(dataset, file_path):\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(dataset, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e28f951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables for each command line argument\n",
    "ehr_data_dir = \"YOUR_PATH/physionet.org/files/mimiciv/1.0/\"\n",
    "cxr_data_dir = \"YOUR_PATH/physionet.org/files/mimic-cxr-jpg/2.0.0\"\n",
    "discharge_path = \"YOUR_PATH/mimic_iv/note/discharge.csv\"\n",
    "rad_path = \"YOUR_PATH/mimic_iv/note/radiology.csv\"\n",
    "core_dir = \"YOUR_PATH/physionet.org/files/mimiciv/1.0/core/\"\n",
    "config_file_path = \"discretizer_config.json\" #mimic4extract/mimic3models/resources/, from MedFuse repo\n",
    "\n",
    "with open(config_file_path, 'r') as file:\n",
    "    config = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7f240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = cxr_data_dir\n",
    "cxr_metadata = pd.read_csv(f'{data_dir}/mimic-cxr-2.0.0-metadata.csv')\n",
    "icu_stay_metadata = pd.read_csv(f'{ehr_data_dir}/per_subject/all_stays.csv')\n",
    "columns = ['subject_id', 'stay_id', 'intime', 'outtime', 'hadm_id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9ce6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cxr_merged_icustays = cxr_metadata.merge(icu_stay_metadata[columns ], how='inner', on='subject_id')\n",
    "cxr_merged_icustays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a03527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine study date time\n",
    "cxr_merged_icustays = cxr_metadata.merge(icu_stay_metadata[columns ], how='inner', on='subject_id')\n",
    "cxr_merged_icustays['StudyTime'] = cxr_merged_icustays['StudyTime'].apply(lambda x: f'{int(float(x)):06}' )\n",
    "cxr_merged_icustays['StudyDateTime'] = pd.to_datetime(cxr_merged_icustays['StudyDate'].astype(str) + ' ' + cxr_merged_icustays['StudyTime'].astype(str) ,format=\"%Y%m%d %H%M%S\")\n",
    "\n",
    "cxr_merged_icustays.intime=pd.to_datetime(cxr_merged_icustays.intime)\n",
    "cxr_merged_icustays.outtime=pd.to_datetime(cxr_merged_icustays.outtime)\n",
    "end_time = cxr_merged_icustays.outtime\n",
    "#if task == 'in-hospital-mortality':\n",
    "#    end_time = cxr_merged_icustays.intime + pd.DateOffset(hours=48)\n",
    "\n",
    "cxr_merged_icustays_during = cxr_merged_icustays.loc[(cxr_merged_icustays.StudyDateTime>=cxr_merged_icustays.intime)&((cxr_merged_icustays.StudyDateTime<=end_time))]\n",
    "\n",
    "# cxr_merged_icustays_during = cxr_merged_icustays.loc[(cxr_merged_icustays.StudyDateTime>=cxr_merged_icustays.intime)&((cxr_merged_icustays.StudyDateTime<=cxr_merged_icustays.outtime))]\n",
    "# select cxrs with the ViewPosition == 'AP\n",
    "cxr_merged_icustays_AP = cxr_merged_icustays_during[cxr_merged_icustays_during['ViewPosition'] == 'AP']\n",
    "\n",
    "groups = cxr_merged_icustays_AP.groupby('stay_id')\n",
    "\n",
    "groups_selected = []\n",
    "for group in groups:\n",
    "    # select the latest cxr for the icu stay\n",
    "    selected = group[1].sort_values('StudyDateTime').tail(1).reset_index()\n",
    "    groups_selected.append(selected)\n",
    "groups = pd.concat(groups_selected, ignore_index=True)\n",
    "\n",
    "paths = glob.glob(cxr_data_dir + \"/resized/\" + '*.jpg')\n",
    "groups[\"dicom_id_path\"] = cxr_data_dir + \"/resized/\" + groups[\"dicom_id\"] + \".jpg\"\n",
    "groups = groups[groups[\"dicom_id_path\"].isin(paths)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acff254c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.read_csv(discharge_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c2e968",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['text'] = ds['text'].apply(lambda x: x.split('Discharge Diagnosis')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b7c10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rad = pd.read_csv(rad_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c3c797",
   "metadata": {},
   "outputs": [],
   "source": [
    "rad.head()[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60b8144",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = create_demo_general(core_dir)\n",
    "demo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2679a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "task = \"phenotyping\"\n",
    "test_data = pd.read_csv(f'{ehr_data_dir}/{task}/{\"test\"}_listfile.csv')\n",
    "val_data = pd.read_csv(f'{ehr_data_dir}/{task}/{\"val\"}_listfile.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86479e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([test_data, val_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368b73ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.merge(groups, on = [\"stay_id\"])\n",
    "data= data.merge(ds, on = [\"subject_id\", \"hadm_id\"])\n",
    "data= data.merge(rad, on = [\"subject_id\", \"hadm_id\"])\n",
    "data= data.merge(demo, on = [\"subject_id\", \"hadm_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21f8260",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('YOUR_PATH/physionet.org/files/mimiciv/1.0/finetune_pheno_data_all_modalities.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec9ed03",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = [\n",
    "       'Acute and unspecified renal failure', 'Acute cerebrovascular disease',\n",
    "       'Acute myocardial infarction', 'Cardiac dysrhythmias',\n",
    "       'Chronic kidney disease',\n",
    "       'Chronic obstructive pulmonary disease and bronchiectasis',\n",
    "       'Complications of surgical procedures or medical care',\n",
    "       'Conduction disorders', 'Congestive heart failure; nonhypertensive',\n",
    "       'Coronary atherosclerosis and other heart disease',\n",
    "       'Diabetes mellitus with complications',\n",
    "       'Diabetes mellitus without complication',\n",
    "       'Disorders of lipid metabolism', 'Essential hypertension',\n",
    "       'Fluid and electrolyte disorders', 'Gastrointestinal hemorrhage',\n",
    "       'Hypertension with complications and secondary hypertension',\n",
    "       'Other liver diseases', 'Other lower respiratory disease',\n",
    "       'Other upper respiratory disease',\n",
    "       'Pleurisy; pneumothorax; pulmonary collapse',\n",
    "       'Pneumonia (except that caused by tuberculosis or sexually transmitted disease)',\n",
    "       'Respiratory failure; insufficiency; arrest (adult)',\n",
    "       'Septicemia (except in labor)', 'Shock'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f829f242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'stay_id' is a column in the DataFrame\n",
    "stay_ids = data['stay_id'].unique()\n",
    "\n",
    "# Shuffle the array of unique stay_ids\n",
    "np.random.shuffle(stay_ids)\n",
    "\n",
    "# Determine the indices to split stay_ids into train, validation, and test sets\n",
    "train_split_index = int(0.7 * len(stay_ids))  # 70% for training\n",
    "val_split_index = int(0.85 * len(stay_ids))  # Next 15% for validation, leaving 15% for test\n",
    "\n",
    "# Select stay_ids for train, validation, and test sets\n",
    "train_stay_ids = stay_ids[:train_split_index]\n",
    "val_stay_ids = stay_ids[train_split_index:val_split_index]\n",
    "test_stay_ids = stay_ids[val_split_index:]\n",
    "\n",
    "# Filter the DataFrame based on the selected stay_ids for each set\n",
    "train_data = data[data['stay_id'].isin(train_stay_ids)]\n",
    "val_data = data[data['stay_id'].isin(val_stay_ids)]\n",
    "test_data = data[data['stay_id'].isin(test_stay_ids)]\n",
    "\n",
    "# Reset the index for each DataFrame to have a clean start from 0\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "val_data = val_data.reset_index(drop=True)\n",
    "test_data = test_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8a58bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv(\"finetune_train_data.csv\", index=False)\n",
    "val_data.to_csv(\"finetune_val_data.csv\", index=False)\n",
    "test_data.to_csv(\"finetune_test_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad566953",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"finetune_pheno_train_data.csv\")\n",
    "val_data = pd.read_csv(\"finetune_pheno_val_data.csv\")\n",
    "test_data = pd.read_csv(\"finetune_pheno_test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e4a067",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_data[CLASSES].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d101a764",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data[CLASSES].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f84a5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_contrast = pd.read_csv(\"train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb8d68f-3bc3-4fdf-be2d-1b7a88fad2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE ADAPTED FROM MEDFUSE REPO\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import platform\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "class Discretizer:\n",
    "    def __init__(self, timestep=0.8, store_masks=True, impute_strategy='zero', start_time='zero',\n",
    "                 config_path= 'discretizer_config.json'):\n",
    "\n",
    "        with open(config_path) as f:\n",
    "            config = json.load(f)\n",
    "            self._id_to_channel = config['id_to_channel']\n",
    "            self._channel_to_id = dict(zip(self._id_to_channel, range(len(self._id_to_channel))))\n",
    "            self._is_categorical_channel = config['is_categorical_channel']\n",
    "            self._possible_values = config['possible_values']\n",
    "            self._normal_values = config['normal_values']\n",
    "\n",
    "        self._header = [\"Hours\"] + self._id_to_channel\n",
    "        self._timestep = timestep\n",
    "        self._store_masks = store_masks\n",
    "        self._start_time = start_time\n",
    "        self._impute_strategy = impute_strategy\n",
    "\n",
    "        # for statistics\n",
    "        self._done_count = 0\n",
    "        self._empty_bins_sum = 0\n",
    "        self._unused_data_sum = 0\n",
    "\n",
    "    def transform(self, X, header=None, end=None):\n",
    "        if header is None:\n",
    "            header = self._header\n",
    "        assert header[0] == \"Hours\"\n",
    "        eps = 1e-6\n",
    "\n",
    "        N_channels = len(self._id_to_channel)\n",
    "        ts = [float(row[0]) for row in X]\n",
    "        for i in range(len(ts) - 1):\n",
    "            assert ts[i] < ts[i+1] + eps\n",
    "\n",
    "        if self._start_time == 'relative':\n",
    "            first_time = ts[0]\n",
    "        elif self._start_time == 'zero':\n",
    "            first_time = 0\n",
    "        else:\n",
    "            raise ValueError(\"start_time is invalid\")\n",
    "\n",
    "        if end is None:\n",
    "            max_hours = max(ts) - first_time\n",
    "        else:\n",
    "            max_hours = end - first_time\n",
    "\n",
    "        N_bins = int(max_hours / self._timestep + 1.0 - eps)\n",
    "\n",
    "        cur_len = 0\n",
    "        begin_pos = [0 for i in range(N_channels)]\n",
    "        end_pos = [0 for i in range(N_channels)]\n",
    "        for i in range(N_channels):\n",
    "            channel = self._id_to_channel[i]\n",
    "            begin_pos[i] = cur_len\n",
    "            if self._is_categorical_channel[channel]:\n",
    "                end_pos[i] = begin_pos[i] + len(self._possible_values[channel])\n",
    "            else:\n",
    "                end_pos[i] = begin_pos[i] + 1\n",
    "            cur_len = end_pos[i]\n",
    "\n",
    "        data = np.zeros(shape=(N_bins, cur_len), dtype=float)\n",
    "        mask = np.zeros(shape=(N_bins, N_channels), dtype=int)\n",
    "        original_value = [[\"\" for j in range(N_channels)] for i in range(N_bins)]\n",
    "        total_data = 0\n",
    "        unused_data = 0\n",
    "\n",
    "        def write(data, bin_id, channel, value, begin_pos):\n",
    "            channel_id = self._channel_to_id[channel]\n",
    "            if self._is_categorical_channel[channel]:\n",
    "                category_id = self._possible_values[channel].index(value)\n",
    "                N_values = len(self._possible_values[channel])\n",
    "                one_hot = np.zeros((N_values,))\n",
    "                one_hot[category_id] = 1\n",
    "                for pos in range(N_values):\n",
    "                    data[bin_id, begin_pos[channel_id] + pos] = one_hot[pos]\n",
    "            else:\n",
    "                data[bin_id, begin_pos[channel_id]] = float(value)\n",
    "\n",
    "        for row in X:\n",
    "            t = float(row[0]) - first_time\n",
    "            if t > max_hours + eps:\n",
    "                continue\n",
    "            bin_id = int(t / self._timestep - eps)\n",
    "            assert 0 <= bin_id < N_bins\n",
    "\n",
    "            for j in range(1, len(row)):\n",
    "                if row[j] == \"\":\n",
    "                    continue\n",
    "                channel = header[j]\n",
    "                channel_id = self._channel_to_id[channel]\n",
    "\n",
    "                total_data += 1\n",
    "                if mask[bin_id][channel_id] == 1:\n",
    "                    unused_data += 1\n",
    "                mask[bin_id][channel_id] = 1\n",
    "\n",
    "                write(data, bin_id, channel, row[j], begin_pos)\n",
    "                original_value[bin_id][channel_id] = row[j]\n",
    "\n",
    "        # impute missing values\n",
    "\n",
    "        if self._impute_strategy not in ['zero', 'normal_value', 'previous', 'next']:\n",
    "            raise ValueError(\"impute strategy is invalid\")\n",
    "\n",
    "        if self._impute_strategy in ['normal_value', 'previous']:\n",
    "            prev_values = [[] for i in range(len(self._id_to_channel))]\n",
    "            for bin_id in range(N_bins):\n",
    "                for channel in self._id_to_channel:\n",
    "                    channel_id = self._channel_to_id[channel]\n",
    "                    if mask[bin_id][channel_id] == 1:\n",
    "                        prev_values[channel_id].append(original_value[bin_id][channel_id])\n",
    "                        continue\n",
    "                    if self._impute_strategy == 'normal_value':\n",
    "                        imputed_value = self._normal_values[channel]\n",
    "                    if self._impute_strategy == 'previous':\n",
    "                        if len(prev_values[channel_id]) == 0:\n",
    "                            imputed_value = self._normal_values[channel]\n",
    "                        else:\n",
    "                            imputed_value = prev_values[channel_id][-1]\n",
    "                    write(data, bin_id, channel, imputed_value, begin_pos)\n",
    "\n",
    "        if self._impute_strategy == 'next':\n",
    "            prev_values = [[] for i in range(len(self._id_to_channel))]\n",
    "            for bin_id in range(N_bins-1, -1, -1):\n",
    "                for channel in self._id_to_channel:\n",
    "                    channel_id = self._channel_to_id[channel]\n",
    "                    if mask[bin_id][channel_id] == 1:\n",
    "                        prev_values[channel_id].append(original_value[bin_id][channel_id])\n",
    "                        continue\n",
    "                    if len(prev_values[channel_id]) == 0:\n",
    "                        imputed_value = self._normal_values[channel]\n",
    "                    else:\n",
    "                        imputed_value = prev_values[channel_id][-1]\n",
    "                    write(data, bin_id, channel, imputed_value, begin_pos)\n",
    "\n",
    "        empty_bins = np.sum([1 - min(1, np.sum(mask[i, :])) for i in range(N_bins)])\n",
    "        self._done_count += 1\n",
    "        self._empty_bins_sum += empty_bins / (N_bins + eps)\n",
    "        self._unused_data_sum += unused_data / (total_data + eps)\n",
    "\n",
    "        if self._store_masks:\n",
    "            data = np.hstack([data, mask.astype(np.float32)])\n",
    "\n",
    "        # create new header\n",
    "        new_header = []\n",
    "        for channel in self._id_to_channel:\n",
    "            if self._is_categorical_channel[channel]:\n",
    "                values = self._possible_values[channel]\n",
    "                for value in values:\n",
    "                    new_header.append(channel + \"->\" + value)\n",
    "            else:\n",
    "                new_header.append(channel)\n",
    "\n",
    "        if self._store_masks:\n",
    "            for i in range(len(self._id_to_channel)):\n",
    "                channel = self._id_to_channel[i]\n",
    "                new_header.append(\"mask->\" + channel)\n",
    "\n",
    "        new_header = \",\".join(new_header)\n",
    "\n",
    "        return (data, new_header)\n",
    "\n",
    "    def print_statistics(self):\n",
    "        print(\"statistics of discretizer:\")\n",
    "        print(\"\\tconverted {} examples\".format(self._done_count))\n",
    "        print(\"\\taverage unused data = {:.2f} percent\".format(100.0 * self._unused_data_sum / self._done_count))\n",
    "        print(\"\\taverage empty  bins = {:.2f} percent\".format(100.0 * self._empty_bins_sum / self._done_count))\n",
    "\n",
    "\n",
    "class Normalizer:\n",
    "    def __init__(self, fields=None):\n",
    "        self._means = None\n",
    "        self._stds = None\n",
    "        self._fields = None\n",
    "        if fields is not None:\n",
    "            self._fields = [col for col in fields]\n",
    "\n",
    "        self._sum_x = None\n",
    "        self._sum_sq_x = None\n",
    "        self._count = 0\n",
    "\n",
    "    def _feed_data(self, x):\n",
    "        x = np.array(x)\n",
    "        self._count += x.shape[0]\n",
    "        if self._sum_x is None:\n",
    "            self._sum_x = np.sum(x, axis=0)\n",
    "            self._sum_sq_x = np.sum(x**2, axis=0)\n",
    "        else:\n",
    "            self._sum_x += np.sum(x, axis=0)\n",
    "            self._sum_sq_x += np.sum(x**2, axis=0)\n",
    "\n",
    "    def _save_params(self, save_file_path):\n",
    "        eps = 1e-7\n",
    "        with open(save_file_path, \"wb\") as save_file:\n",
    "            N = self._count\n",
    "            self._means = 1.0 / N * self._sum_x\n",
    "            self._stds = np.sqrt(1.0/(N - 1) * (self._sum_sq_x - 2.0 * self._sum_x * self._means + N * self._means**2))\n",
    "            self._stds[self._stds < eps] = eps\n",
    "            pickle.dump(obj={'means': self._means,\n",
    "                             'stds': self._stds},\n",
    "                        file=save_file,\n",
    "                        protocol=2)\n",
    "\n",
    "    def load_params(self, load_file_path):\n",
    "        with open(load_file_path, \"rb\") as load_file:\n",
    "            if platform.python_version()[0] == '2':\n",
    "                dct = pickle.load(load_file)\n",
    "            else:\n",
    "                dct = pickle.load(load_file, encoding='latin1')\n",
    "            self._means = dct['means']\n",
    "            self._stds = dct['stds']\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self._fields is None:\n",
    "            fields = range(X.shape[1])\n",
    "        else:\n",
    "            fields = self._fields\n",
    "        ret = 1.0 * X\n",
    "        for col in fields:\n",
    "            ret[:, col] = (X[:, col] - self._means[col]) / self._stds[col]\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a23f05c-b30c-496c-af62-9e6173ea6db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_timeseries(ehr_data_dir, task):\n",
    "    path = f'{ehr_data_dir}/{task}/train/10002430_episode1_timeseries.csv'\n",
    "    ret = []\n",
    "    with open(path, \"r\") as tsfile:\n",
    "        header = tsfile.readline().strip().split(',')\n",
    "        assert header[0] == \"Hours\"\n",
    "        for line in tsfile:\n",
    "            mas = line.strip().split(',')\n",
    "            ret.append(np.array(mas))\n",
    "    return np.stack(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3797565-a681-40ab-81f2-829eb06d1458",
   "metadata": {},
   "outputs": [],
   "source": [
    "discretizer = Discretizer(timestep=1.0,\n",
    "                          store_masks=True,\n",
    "                          impute_strategy='previous',\n",
    "                          start_time='zero')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a7ac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "task = \"phenotyping\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fbb6ad-d44b-4ecb-bb94-b3cf0104527d",
   "metadata": {},
   "outputs": [],
   "source": [
    "discretizer_header = discretizer.transform(read_timeseries(ehr_data_dir, task))[1].split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a69c44-9f7c-4be4-beba-17b16c611ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_channels = [i for (i, x) in enumerate(discretizer_header) if x.find(\"->\") == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569e78b8-9c4f-44b7-a652-3e3c221f09e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3c7765-fd28-4758-9774-c0a957bca39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer(fields=cont_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9d7d2f-65b0-4a5e-8bee-41abff7acfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer_state = \"ph_ts1.0.input_str_previous.start_time_zero.normalizer\"\n",
    "normalizer.load_params(normalizer_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40502c7-2537-41e5-815e-bb59bc8c8486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "def _read_timeseries(ts_filename, dataset_dir, time_bound=None):\n",
    "    ret = []\n",
    "    with open(os.path.join(dataset_dir, ts_filename), \"r\") as tsfile:\n",
    "        header = tsfile.readline().strip().split(',')\n",
    "        assert header[0] == \"Hours\"\n",
    "        for line in tsfile:\n",
    "            mas = line.strip().split(',')\n",
    "            if time_bound is not None:\n",
    "                t = float(mas[0])\n",
    "                if t > time_bound + 1e-6:\n",
    "                    break\n",
    "            ret.append(mas)  # Keep as list of strings for flexibility\n",
    "    return (ret, header)\n",
    "\n",
    "def preprocess_and_save(discretizer, normalizer, listfile, base_dataset_dir, output_dir):\n",
    "    df = pd.read_csv(listfile)\n",
    "    for idx, row in df.iterrows():\n",
    "        filename = row['stay']\n",
    "        \n",
    "        # Check for the file's existence in both 'train' and 'test' directories\n",
    "        train_dir = os.path.join(base_dataset_dir, 'train')\n",
    "        test_dir = os.path.join(base_dataset_dir, 'test')\n",
    "        if os.path.exists(os.path.join(train_dir, filename)):\n",
    "            dataset_dir = train_dir\n",
    "        elif os.path.exists(os.path.join(test_dir, filename)):\n",
    "            dataset_dir = test_dir\n",
    "        else:\n",
    "            print(f\"File not found in both train and test directories: {filename}\")\n",
    "            continue  # Skip this file if not found\n",
    "        \n",
    "        data, header = _read_timeseries(filename, dataset_dir, None)\n",
    "        \n",
    "        print(f\"Original length of {filename}: {len(data)}\")\n",
    "        \n",
    "        # Preprocess and save as before\n",
    "        data_preprocessed = discretizer.transform(data, header=header)[0]\n",
    "        data_normalized = normalizer.transform(data_preprocessed)\n",
    "        \n",
    "        print(f\"Length after preprocessing {filename}: {len(data_normalized)}\")\n",
    "        \n",
    "        output_path = os.path.join(output_dir, os.path.basename(filename).replace('.csv', '.h5'))\n",
    "        with h5py.File(output_path, 'w') as hf:\n",
    "            hf.create_dataset('data', data=data_normalized)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba75bac-de87-4197-b019-62027778c8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EHRdatasetFinetune(Dataset):\n",
    "    def __init__(self, listfile, preprocessed_dir, classes):\n",
    "        self.data_files = []\n",
    "        self.labels = []\n",
    "        \n",
    "        df = pd.read_csv(listfile)\n",
    "        for idx, row in df.iterrows():\n",
    "            # Load preprocessed data\n",
    "            preprocessed_file = os.path.join(preprocessed_dir, os.path.basename(row['stay']).replace('.csv', '.h5'))\n",
    "            if os.path.exists(preprocessed_file):\n",
    "                self.data_files.append(preprocessed_file)\n",
    "                self.labels.append(row[classes].values)  # Adjust based on your actual label handling\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        try:\n",
    "            with h5py.File(self.data_files[index], 'r') as hf:\n",
    "                data = hf['data'][:]\n",
    "            label = self.labels[index]  # Adjust this based on how you handle labels\n",
    "            return data, label\n",
    "        \n",
    "        except KeyError as e:\n",
    "            print(f\"Error loading data from file {self.data_files[index]}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93db5296",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = os.path.join(ehr_data_dir, f'{task}')\n",
    "train_listfile = \"finetune_pheno_train_ts_df.csv\"\n",
    "output_dir = 'YOUR_PATH/timeseries_data_finetune_pheno'\n",
    "\n",
    "\n",
    "preprocess_and_save(discretizer, normalizer, train_listfile , dataset_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3558c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_listfile = \"finetune_pheno_test_ts_df.csv\"\n",
    "output_dir = 'YOUR_PATH/timeseries_data_finetune_pheno'\n",
    "\n",
    "\n",
    "preprocess_and_save(discretizer, normalizer,test_listfile , dataset_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924f100e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_listfile = \"finetune_pheno_val_ts_df.csv\"\n",
    "output_dir = 'YOUR_PATH/timeseries_data_finetune_pheno'\n",
    "\n",
    "preprocess_and_save(discretizer, normalizer,val_listfile , dataset_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b458c7b1-ac77-4bbc-88ec-0e123182cdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EHRdatasetFinetune(train_listfile, output_dir, CLASSES)\n",
    "test_dataset = EHRdatasetFinetune(test_listfile, output_dir, CLASSES)\n",
    "val_dataset = EHRdatasetFinetune(val_listfile, output_dir, CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c356de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transposed_labels = list(zip(*val_dataset.labels))\n",
    "\n",
    "# Count the number of 1's in each column\n",
    "ones_count_per_index = [sum(column) for column in transposed_labels]\n",
    "\n",
    "ones_count_per_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcbc148-186f-4376-aa36-3533d47128fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "\n",
    "def my_collate(batch):\n",
    "    data = [torch.tensor(item[0], dtype=torch.float) for item in batch]\n",
    "    labels = [item[1] for item in batch]\n",
    "    \n",
    "    # Diagnostic print: Sequence lengths before padding\n",
    "    print(\"Before padding:\", [len(x) for x in data])\n",
    "    \n",
    "    data_padded = pad_sequence(data, batch_first=True, padding_value=0.0)\n",
    "    lengths = torch.tensor([len(x) for x in data], dtype=torch.long)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    # Diagnostic print: Sequence lengths after padding (should be the same within a batch)\n",
    "    print(\"After padding:\", data_padded.shape[1])\n",
    "    \n",
    "    return data_padded, lengths, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead08e9a-54e3-4e62-8859-30572277fc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, collate_fn=my_collate, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f6180a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b751b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is done first for train so one-hot-encoding is the same for val and for testing as it is for train\n",
    "unique_categories, fixed_length = determine_categories_and_length(ehr_data_dir, task, \"train\", train_data, config)\n",
    "encoders = create_encoders(unique_categories)\n",
    "categorical_cols = ['admission_type', 'admission_location', 'insurance', 'language', 'marital_status', 'ethnicity', 'gender', 'anchor_year_group']\n",
    "imputer_continuous = SimpleImputer(strategy='mean')\n",
    "imputer_categorical = SimpleImputer(strategy='most_frequent')\n",
    "#train_data[continuous_cols] = imputer_continuous.fit_transform(train_data[continuous_cols])\n",
    "df = train_data\n",
    "for col in categorical_cols:\n",
    "    print(col)\n",
    "    transform = imputer_categorical.fit_transform(train_data[[col]])\n",
    "    print(np.array(transform).shape)\n",
    "    print(transform)\n",
    "    print(transform.ravel())\n",
    "    print(transform.ravel().shape)\n",
    "    print(df[col])\n",
    "    df[col] = transform.ravel()\n",
    "    #end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eaaa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is only relevant for the demographic modality\n",
    "_, demo_train_encoders = preprocess_demo(train_data_contrast, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66e97b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_train_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d85636",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_train_data, _ = preprocess_demo(train_data, demo_train_encoders)\n",
    "demo_dataset = create_demo_dataset(demo_train_data, train_data[CLASSES].values)\n",
    "save_dataset(demo_dataset, f'{ehr_data_dir}/{task}/train_finetune_demo_dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3320f695",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_val_data, _ = preprocess_demo(val_data, demo_train_encoders)\n",
    "demo_test_data, _ = preprocess_demo(test_data, demo_train_encoders)\n",
    "\n",
    "demo_dataset_val = create_demo_dataset(demo_val_data, val_data[CLASSES].values)\n",
    "demo_dataset_test = create_demo_dataset(demo_test_data, test_data[CLASSES].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e8904d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataset(demo_dataset_val, f'{ehr_data_dir}/{task}/val_finetune_demo_dataset.pkl')\n",
    "save_dataset(demo_dataset_test, f'{ehr_data_dir}/{task}/test_finetune_demo_dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65c8e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "img_dataset = create_image_dataset(train_data,CLASSES, 'dicom_id_path', transform)\n",
    "save_dataset(img_dataset, f'{ehr_data_dir}/{task}/train_finetune_img_dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287737b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dataset = create_image_dataset(val_data, CLASSES, 'dicom_id_path', transform)\n",
    "save_dataset(img_dataset, f'{ehr_data_dir}/{task}/val_finetune_img_dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dcde3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dataset = create_image_dataset(test_data, CLASSES, 'dicom_id_path', transform)\n",
    "save_dataset(img_dataset, f'{ehr_data_dir}/{task}/test_finetune_img_dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92834fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dataset = create_text_dataset(train_data, CLASSES, \"text_x\")\n",
    "save_dataset(text_dataset, f'{ehr_data_dir}/{task}/train_finetune_text_ds_dataset.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd85bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dataset_val = create_text_dataset(val_data,CLASSES, \"text_x\")\n",
    "save_dataset(text_dataset_val, f'{ehr_data_dir}/{task}/val_finetune_text_ds_dataset.pkl')\n",
    "\n",
    "text_dataset_test = create_text_dataset(test_data,CLASSES, \"text_x\")\n",
    "save_dataset(text_dataset_test, f'{ehr_data_dir}/{task}/test_finetune_text_ds_dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c9ffdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dataset_rad = create_text_dataset(train_data,CLASSES, \"text_y\")\n",
    "save_dataset(text_dataset_rad, f'{ehr_data_dir}/{task}/train_finetune_text_rad_dataset.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fa0644",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dataset_val = create_text_dataset(val_data,CLASSES, \"text_y\")\n",
    "save_dataset(text_dataset_val, f'{ehr_data_dir}/{task}/val_finetune_text_rad_dataset.pkl')\n",
    "\n",
    "text_dataset_test = create_text_dataset(test_data,CLASSES, \"text_y\")\n",
    "save_dataset(text_dataset_test, f'{ehr_data_dir}/{task}/test_finetune_text_rad_dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4abd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataset(train_dataset, f'{ehr_data_dir}/{task}/train_finetune_ts_dataset.pkl')\n",
    "save_dataset(val_dataset, f'{ehr_data_dir}/{task}/val_finetune_ts_dataset.pkl')\n",
    "save_dataset(test_dataset, f'{ehr_data_dir}/{task}/test_finetune_ts_dataset.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
